{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.utils import check_X_y\n",
    "from sklearn.datasets import load_svmlight_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_offsets(arr):\n",
    "    \"\"\"Return a sequence of start/end offsets for the value subgroups in the input\"\"\"\n",
    "    d = np.ones(arr.size, dtype=int)\n",
    "    d[1:] = (arr[:-1] != arr[1:]).astype(int)\n",
    "    idx = np.where(np.append(d, 1))[0]\n",
    "    return zip(idx, idx[1:])\n",
    "\n",
    "\n",
    "class Scorer(object):\n",
    "    def __init__(self, score_func, **kwargs):\n",
    "        self.score_func = score_func\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    def __call__(self, *args):\n",
    "        return self.score_func(*args, **self.kwargs)\n",
    "\n",
    "\n",
    "# DCG/nDCG (Normalized Discounted Cumulative Gain)\n",
    "# https://en.wikipedia.org/wiki/Discounted_cumulative_gain\n",
    "\n",
    "def _burges_dcg(y_true, y_pred, k=None):\n",
    "    # order = np.argsort(y_pred)[::-1]\n",
    "    order = np.argsort(-y_pred)\n",
    "    y_true = np.take(y_true, order[:k])\n",
    "    gain = 2 ** y_true - 1\n",
    "    discounts = np.log2(np.arange(len(gain)) + 2)\n",
    "    return np.sum(gain / discounts)\n",
    "\n",
    "def _dcg_score(y_true, y_pred, qid, k=None, dcg_func=None):\n",
    "    assert dcg_func is not None\n",
    "    y_true = np.maximum(y_true, 0)\n",
    "    return np.array([dcg_func(y_true[a:b], y_pred[a:b], k=k) for a, b in group_offsets(qid)])\n",
    "\n",
    "def _ndcg_score(y_true, y_pred, qid, k=None, dcg_func=None):\n",
    "    assert dcg_func is not None\n",
    "    y_true = np.maximum(y_true, 0)\n",
    "    dcg = _dcg_score(y_true, y_pred, qid, k=k, dcg_func=dcg_func)\n",
    "    idcg = np.array([dcg_func(np.sort(y_true[a:b]), np.arange(0, b - a), k=k)\n",
    "                     for a, b in group_offsets(qid)])\n",
    "    assert (dcg <= idcg).all()\n",
    "    idcg[idcg == 0] = 1\n",
    "    return dcg / idcg\n",
    "\n",
    "def ndcg_score(y_true, y_pred, qid, k=None):\n",
    "    dcg_func = _burges_dcg \n",
    "    return _ndcg_score(y_true, y_pred, qid, k=k, dcg_func=dcg_func)\n",
    "\n",
    "class NDCGScorer(Scorer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(NDCGScorer, self).__init__(ndcg_score, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaRank(sklearn.base.BaseEstimator):\n",
    "    \"\"\"AdaRank algorithm\"\"\"\n",
    "\n",
    "    def __init__(self, max_iter=500, tol=0.0001, estop=1, verbose=False, scorer=None):\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.estop = estop\n",
    "        self.verbose = verbose\n",
    "        self.scorer = scorer\n",
    "\n",
    "    def fit(self, X, y, qid, X_valid=None, y_valid=None, qid_valid=None):\n",
    "        \"\"\"Fit a model to the data\"\"\"\n",
    "        X, y = check_X_y(X, y, 'csr')\n",
    "        X = X.toarray()\n",
    "\n",
    "        if X_valid is None:\n",
    "            X_valid, y_valid, qid_valid = X, y, qid\n",
    "        else:\n",
    "            X_valid, y_valid = check_X_y(X_valid, y_valid, 'csr')\n",
    "            X_valid = X_valid.toarray()\n",
    "\n",
    "        n_queries = np.unique(qid).shape[0]\n",
    "        weights = np.ones(n_queries, dtype=np.float64) / n_queries\n",
    "        weak_rankers = []\n",
    "        coef = np.zeros(X.shape[1])\n",
    "\n",
    "        # use nDCG@10 as the default scorer\n",
    "        if self.scorer is None:\n",
    "            self.scorer = NDCGScorer(k=10)\n",
    "\n",
    "        # precompute performance measurements for all weak rankers\n",
    "        weak_ranker_score = []\n",
    "        for j in range(X.shape[1]):\n",
    "            pred = X[:, j].ravel()\n",
    "            weak_ranker_score.append(self.scorer(y, pred, qid))\n",
    "\n",
    "        best_perf_train = -np.inf\n",
    "        best_perf_valid = -np.inf\n",
    "        used_fids = []\n",
    "        estop = None\n",
    "\n",
    "        self.n_iter = 0\n",
    "        while self.n_iter < self.max_iter:\n",
    "            self.n_iter += 1\n",
    "\n",
    "            best_weighted_average = -np.inf\n",
    "            best_weak_ranker = None\n",
    "            for fid, score in enumerate(weak_ranker_score):\n",
    "                if fid in used_fids:\n",
    "                    continue\n",
    "                weighted_average = np.dot(weights, score)\n",
    "                if weighted_average > best_weighted_average:\n",
    "                    best_weak_ranker = {'fid': fid, 'score': score}\n",
    "                    best_weighted_average = weighted_average\n",
    "\n",
    "            # stop when all the weaker rankers are out\n",
    "            if best_weak_ranker is None:\n",
    "                break\n",
    "\n",
    "            h = best_weak_ranker\n",
    "            h['alpha'] = 0.5 * (math.log(np.dot(weights, 1 + h['score']) /\n",
    "                                         np.dot(weights, 1 - h['score'])))\n",
    "            weak_rankers.append(h)\n",
    "\n",
    "            # update the ranker\n",
    "            coef[h['fid']] += h['alpha']\n",
    "\n",
    "            # if len(used_fids) > 5:\n",
    "            #     used_fids.pop(0)\n",
    "            # used_fids.append(h['fid'])\n",
    "\n",
    "            # score both training and validation data\n",
    "            score_train = self.scorer(y, np.dot(X, coef), qid)\n",
    "            perf_train = score_train.mean()\n",
    "\n",
    "            perf_valid = perf_train\n",
    "            if X_valid is not X:\n",
    "                perf_valid = self.scorer(y_valid, np.dot(X_valid, coef), qid_valid).mean()\n",
    "\n",
    "            if self.verbose:\n",
    "                print('{n_iter}\\t{alpha}\\t{fid}\\t{score}\\ttrain {train:.4f}\\tvalid {valid:.4f}'.\n",
    "                      format(n_iter=self.n_iter, alpha=h['alpha'], fid=h['fid'],\n",
    "                             score=h['score'][:5], train=perf_train, valid=perf_valid),\n",
    "                      file=sys.stderr)\n",
    "\n",
    "            # update the best validation scores\n",
    "            if perf_valid > best_perf_valid + self.tol:\n",
    "                estop = 0\n",
    "                best_perf_valid = perf_valid\n",
    "                self.coef_ = coef.copy()\n",
    "            else:\n",
    "                estop += 1\n",
    "\n",
    "            # update the best training score\n",
    "            if perf_train > best_perf_train + self.tol:\n",
    "                best_perf_train = perf_train\n",
    "            else:\n",
    "                # stop if scores on both sets fail to improve\n",
    "                if estop >= self.estop:\n",
    "                    break\n",
    "\n",
    "            # update weights\n",
    "            new_weights = np.exp(-score_train)\n",
    "            weights = new_weights / new_weights.sum()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X, qid):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        return np.dot(X.toarray(), self.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, qid = load_svmlight_file(\"datasets/mq2007/train.txt\", query_id=True)\n",
    "    \n",
    "X_test, y_test, qid_test = load_svmlight_file(\"datasets/mq2007/test.txt\", query_id=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper params\n",
    "k = 10\n",
    "max_iter = 100\n",
    "patience = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nDCG@1\t0.40773809523809523\n",
      "nDCG@2\t0.39950776144698336\n",
      "nDCG@3\t0.4079126410276921\n",
      "nDCG@4\t0.4069481096852681\n",
      "nDCG@5\t0.41673937966570745\n",
      "nDCG@10\t0.4502928537269599\n",
      "nDCG@20\t0.5065914762962426\n"
     ]
    }
   ],
   "source": [
    "model = AdaRank(max_iter=max_iter,\n",
    "                    estop=patience,\n",
    "                    verbose=False,\n",
    "                    scorer=NDCGScorer(k=k))\n",
    "\n",
    "model.fit(X, y, qid)\n",
    "\n",
    "predictions = model.predict(X_test, qid_test)\n",
    "for k in (1, 2, 3, 4, 5, 10, 20):\n",
    "        score = NDCGScorer(k=k)(y_test, predictions, qid_test).mean()\n",
    "        print('nDCG@{}\\t{}'.format(k, score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
